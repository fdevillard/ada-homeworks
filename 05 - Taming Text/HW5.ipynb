{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "### 1 Word clouds\n",
    "\n",
    "Let first look at some raw wordclouds. We build 2 of them:\n",
    "- RawText column from the data\n",
    "- ExtractedSubject and ExtractedBodyText columns concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv('hillary-clinton-emails/Emails.csv')\n",
    "raw_corpus = '\\n'.join(emails.RawText)\n",
    "extracted_text = emails.ExtractedSubject.fillna('\\n') + emails.ExtractedBodyText.fillna('\\n')\n",
    "extracted_emails = pd.DataFrame(extracted_text, columns=['Text'])\n",
    "extracted_corpus = '\\n'.join(extracted_emails.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(raw_corpus)\n",
    "wordcloud_extracted = WordCloud().generate(extracted_corpus)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,15))\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Raw corpus')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(wordcloud_extracted)\n",
    "ax2.set_title('Extracted text')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the RawText contains a lot of annotations (e.g. \"UNCLASSIFIED U.S. Department of State Case No.\").  \n",
    "But the extracted text contains more short keywords (e.g. \"Fw:\", \"Re:\", etc.)\n",
    "\n",
    "Now let create a pipeline:\n",
    "- tokenize sentence and words\n",
    "- remove stopwords (plus some more related to emails)\n",
    "- stemming (using the Porter algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Don't forget to run nltk.download() if not already done...\n",
    "\n",
    "def tokenize(str):\n",
    "    return [nltk.word_tokenize(s) for s in nltk.sent_tokenize(str)]\n",
    "\n",
    "stopwords = stopwords.words('english') + ['fvv','fw','fwd','re','am','pm','n\\'t','\\'s']\n",
    "def remove_stopwords(words):\n",
    "    return [w for w in words if w.lower() not in stopwords]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    return [w for w in words if w not in string.punctuation]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "def stemming(words):\n",
    "    return [porter.stem(word) for word in words]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmantizing(stemms) :\n",
    "    return [lemmatizer.lemmatize(stemm) for stemm in stemms]\n",
    "\n",
    "def run_pipeline(str, \n",
    "                 return_as_str=False, \n",
    "                 do_remove_stopwords=True, \n",
    "                 do_stemming=True,\n",
    "                 do_lemmantizing=False,\n",
    "                 do_remove_punctuation=False):\n",
    "    l = []\n",
    "    words = []\n",
    "    sentences = tokenize(str)\n",
    "    for sentence in sentences:\n",
    "        if do_remove_stopwords:\n",
    "            words = remove_stopwords(sentence)\n",
    "        else:\n",
    "            words = sentence\n",
    "        if do_remove_punctuation:\n",
    "            words = remove_punctuation(words)\n",
    "        if do_stemming:\n",
    "            words = stemming(words)\n",
    "        if do_lemmantizing:\n",
    "            words = lemmantizing(words)\n",
    "        if return_as_str:\n",
    "            l.append(' '.join(words))\n",
    "        else:\n",
    "            l.append(words)\n",
    "    if return_as_str:\n",
    "        return ' '.join(l)\n",
    "    else:\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    processed_raw = np.load('processed_raw.npy').item()\n",
    "    processed_extracted = np.load('processed_extracted.npy').item()\n",
    "except:\n",
    "    processed_raw = run_pipeline(raw_corpus, return_as_str=True)\n",
    "    np.save('processed_raw', processed_raw)\n",
    "    processed_extracted = run_pipeline(extracted_corpus, return_as_str=True)\n",
    "    np.save('processed_extracted', processed_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(processed_raw)\n",
    "wordcloud_extracted = WordCloud().generate(processed_extracted)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,15))\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Raw corpus')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(wordcloud_extracted)\n",
    "ax2.set_title('Extracted text')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word clouds are very close together.  \n",
    "The first approach is faster and straightforward but it could lack some fine grained tuning regarding language processing.  \n",
    "The second one is way slower as we run through many pre-processing steps. But its main advantage is the ability to tune some parameters (stopwords, stemming, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Finding countries in the whole corpus:\n",
    "\n",
    "__Note:__ We only retrieve countries by their name.  \n",
    "_When trying with abbreviations we get a lot of false results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycountry import countries\n",
    "\n",
    "def get_country(str):\n",
    "    try:\n",
    "        c = countries.get(name=str.title()).name\n",
    "    except:\n",
    "        c = None\n",
    "    return c\n",
    "\n",
    "def retrieve_countries(emails):\n",
    "    mentioned = {}\n",
    "    \n",
    "    def add(country, i):\n",
    "        if country in mentioned:\n",
    "            mentioned[country].append(i)\n",
    "        else:\n",
    "            mentioned[country] = [i]\n",
    "    \n",
    "    for i, email in emails.iteritems():\n",
    "        tokens = nltk.word_tokenize(email)\n",
    "        for token in tokens:\n",
    "            country = get_country(token)\n",
    "            if country:\n",
    "                add(country, i)\n",
    "    return mentioned\n",
    "\n",
    "try:\n",
    "    mentioned = np.load('mentioned.npy').item()\n",
    "    mentioned2 = np.load('mentioned2.npy').item()\n",
    "except:\n",
    "    mentioned = retrieve_countries(emails.RawText)\n",
    "    np.save('mentioned2.npy', mentioned)\n",
    "    mentioned2 = retrieve_countries(extracted_emails.Text)\n",
    "    np.save('mentioned2.npy', mentioned2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the polarities using NLTK vader package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    polarities = np.load('polarities.npy')\n",
    "    polarities2 = np.load('polarities2.npy')\n",
    "except:\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarities = [sid.polarity_scores(email) for email in emails.RawText]\n",
    "    np.save('polarities.npy', polarities)\n",
    "    polarities2 = [sid.polarity_scores(email) for email in extracted_emails.Text]\n",
    "    np.save('polarities2.npy', polarities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for country, emails in mentioned2.items():\n",
    "    for email in set(emails):\n",
    "        polarity = polarities2[email]['compound']\n",
    "        result.append([country, polarity])\n",
    "result = pd.DataFrame(result, columns=['Country','Polarity'])\n",
    "sorted = result.groupby('Country').mean().sort_values(by='Polarity')\n",
    "colors = ['b' if pol > 0 else 'r' for pol in sorted.Polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,25))\n",
    "sns.barplot(x='Polarity', \n",
    "            y='Country', \n",
    "            data=result, \n",
    "            order=sorted.index, \n",
    "            palette=colors, \n",
    "            saturation=.4, \n",
    "            errcolor='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def demo_liu_hu_lexicon(sentence):\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        print('Positive')\n",
    "    elif pos_words < neg_words:\n",
    "        print('Negative')\n",
    "    elif pos_words == neg_words:\n",
    "        print('Neutral')\n",
    "\n",
    "\n",
    "str = \"This movie was actually neither that funny, nor super witty.\"\n",
    "\n",
    "demo_liu_hu_lexicon(str)\n",
    "#demo_vader_instance(str)\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(str)#['compound']\n",
    "#sentiments = [map(sid.polarity_scores, t) for t in emails['RawText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Exploring the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = run_pipeline(extracted_corpus, do_remove_punctuation=True, do_lemmantizing=True, do_stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "dictionary = Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(s) for s in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOPICS = 20\n",
    "lda = LdaModel(corpus, num_topics=N_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topics(num_topics=N_TOPICS, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 BONUS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "#persons = pd.read_csv('hillary-clinton-emails/Persons.csv', header=0, index_col=0)\n",
    "#aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', header=0, index_col=0)\n",
    "receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senders = emails[['Id','SenderPersonId']]\n",
    "senders = senders[pd.notnull(senders.SenderPersonId)]\n",
    "print('%.2f' % (100*len(senders)/len(emails)), '% of emails have a valid sender.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for _, emailId, i in senders.itertuples():\n",
    "    for _, emailId2, j in receivers[receivers.EmailId == emailId].itertuples():\n",
    "        assert emailId == emailId2\n",
    "        G.add_edge(i, j, email=emailId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "nx.draw_random(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graphs = list(nx.connected_component_subgraphs(G))\n",
    "#community_emails = []\n",
    "#for graph in graphs:\n",
    "#    l = []\n",
    "#    for _,_,data in graph.edges_iter(data=True):\n",
    "#        email = emails[emails.Id == data['email']]\n",
    "#        l.append(email.ExtractedSubject.fillna(' ') + email.ExtractedBodyText.fillna(' '))\n",
    "#    community_emails.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "community.best_partition(G) ## ???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
