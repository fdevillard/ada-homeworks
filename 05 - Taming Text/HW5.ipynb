{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "### 1 Word clouds\n",
    "\n",
    "Let first look at a raw word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv('hillary-clinton-emails/Emails.csv')\n",
    "raw_corpus = '\\n'.join(emails['RawText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(raw_corpus)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let create a pipeline and process again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't forget to run nltk.download() if not already done...\n",
    "from nltk.tokenize import RegexpTokenizer #, StanfordTokenizer\n",
    "\n",
    "def tokenize(str):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    #return tokenizer.tokenize(str)\n",
    "    return [tokenizer.tokenize(s) for s in nltk.sent_tokenize(str)]\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "\n",
    "def stemming(words):\n",
    "    l = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "    for word in words:\n",
    "        l.append(porter.stem(word))\n",
    "    return l\n",
    "\n",
    "def pipeline(str, return_as_str=False, do_remove_stopwords=False):\n",
    "    l = []\n",
    "    words = []\n",
    "    sentences = tokenize(str)\n",
    "    for sentence in sentences:\n",
    "        if do_remove_stopwords:\n",
    "            words = remove_stopwords(sentence)\n",
    "        else:\n",
    "            words = sentence\n",
    "        words = stemming(words)\n",
    "        if return_as_str:\n",
    "            l.append(' '.join(words))\n",
    "        else:\n",
    "            l.append(words)\n",
    "    if return_as_str:\n",
    "        return ' '.join(l)\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "#tokenize = lambda email: nltk.word_tokenize(email)\n",
    "#not_stopword = lambda word: word not in stopwords.words('english')\n",
    "# not_punctuation = lambda word: word not in string.punctuation\n",
    "\n",
    "#tokens_list = []\n",
    "#for email in emails['RawText']:\n",
    "#    tokens = pipeline(email, do_remove_stopwords=True)\n",
    "#    tokens_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed = pipeline(raw_corpus, return_as_str=True, do_remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud_2 = WordCloud().generate(stemmed)\n",
    "plt.imshow(wordcloud_2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word clouds are very close together.  \n",
    "The first approach is faster and straightforward but it could lack some fine grained tuning regarding language processing.  \n",
    "The second one is way slower as we run through many pre-processing steps. But its main advantage is the ability to tune some parameters (stop words, stemming, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycountry import countries\n",
    "\n",
    "def get_country(text):\n",
    "    c = None\n",
    "    '''if (len(country) == 2):\n",
    "        c = countries.get(alpha_2=country)\n",
    "    elif (len(country) == 3):\n",
    "        c = countries.get(alpha_3=country)\n",
    "    else:\n",
    "        c = countries.get(name=country.title())'''\n",
    "    if (len(text) > 3):\n",
    "        try:\n",
    "            c = countries.get(name=text.title()).name\n",
    "        except:\n",
    "            pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mentioned = {}\n",
    "for i in range(len(tokens)):\n",
    "    for token in tokens[i]:\n",
    "        country = get_country(token)\n",
    "        if country:\n",
    "            if country in mentioned:\n",
    "                mentioned[country].append(i)\n",
    "            else:\n",
    "                mentioned[country] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = [map(sid.polarity_scores, t) for t in emails['RawText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = {}\n",
    "for country, idx in mentioned.iteritems():\n",
    "    sentiment = []\n",
    "    for i in idx:\n",
    "        sentiment.append(sentiments[i])\n",
    "    total[country] = np.mean(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models.ldamodel\n",
    "\n",
    "lda = LdaModel(corpus, num_topics=10) # 5 to 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
