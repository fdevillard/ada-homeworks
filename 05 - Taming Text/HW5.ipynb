{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "### 1 Word clouds\n",
    "\n",
    "Let first look at some raw wordclouds. We build 2 of them:\n",
    "- RawText column from the data\n",
    "- ExtractedSubject and ExtractedBodyText columns concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_emails = pd.read_csv('hillary-clinton-emails/Emails.csv')\n",
    "raw_corpus = '\\n'.join(all_emails.RawText)\n",
    "extracted_text = all_emails.ExtractedSubject.fillna('\\n') + all_emails.ExtractedBodyText.fillna('\\n')\n",
    "extracted_emails = pd.DataFrame(extracted_text, columns=['Text'])\n",
    "extracted_corpus = '\\n'.join(extracted_emails.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(raw_corpus)\n",
    "wordcloud_extracted = WordCloud().generate(extracted_corpus)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Raw corpus')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(wordcloud_extracted)\n",
    "ax2.set_title('Extracted text')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the RawText contains a lot of annotations (e.g. \"UNCLASSIFIED U.S. Department of State Case No.\").  \n",
    "But the extracted text contains more short keywords (e.g. \"Fw:\", \"Re:\", etc.)\n",
    "\n",
    "Now let create a pipeline:\n",
    "- tokenize sentence and words\n",
    "- remove stopwords (plus some more related to emails)\n",
    "- stemming (using the Porter algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't forget to run nltk.download() if not already done...\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # If using another tokenizer, add: ['n\\'t','\\'s','...','\\'\\'','``','--']\n",
    "    stop_words = stopwords.words('english') + ['fvv','fw','fwd','re','am','pm']\n",
    "    return [w.lower() for w in words if w.lower() not in stop_words]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    return [w for w in words if w not in string.punctuation]\n",
    "\n",
    "def stemming(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    return [porter.stem(w) for w in words]\n",
    "\n",
    "def lemmantizing(words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "def run_pipeline(text,\n",
    "                 return_as_str = False,\n",
    "                 do_remove_stopwords=True, \n",
    "                 do_stemming=False,\n",
    "                 do_lemmantizing=True,\n",
    "                 do_remove_punctuation=True):\n",
    "    words = tokenize(text)\n",
    "    if do_remove_stopwords:\n",
    "        words = remove_stopwords(words)\n",
    "    if do_remove_punctuation:\n",
    "        words = remove_punctuation(words)\n",
    "    if do_stemming:\n",
    "        words = stemming(words)\n",
    "    if do_lemmantizing:\n",
    "        words = lemmantizing(words)\n",
    "    if return_as_str:\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    processed_raw = np.load('processed_raw.npy').item()\n",
    "    processed_extracted = np.load('processed_extracted.npy').item()\n",
    "except:\n",
    "    processed_raw = run_pipeline(raw_corpus, return_as_str=True)\n",
    "    np.save('processed_raw', processed_raw)\n",
    "    processed_extracted = run_pipeline(extracted_corpus, return_as_str=True)\n",
    "    np.save('processed_extracted', processed_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(processed_raw)\n",
    "wordcloud_extracted = WordCloud().generate(processed_extracted)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Raw corpus')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(wordcloud_extracted)\n",
    "ax2.set_title('Extracted text')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two wordclouds are very close together. We also remark that the difference between Raw and Extracted is way smaller in the second approach.  \n",
    "Pros and cons:  \n",
    "\n",
    "- The first approach is much faster and straightforward but it could lack some fine grained tuning regarding language processing.  \n",
    "- The second one is way slower as we run through many pre-processing steps. But its main advantage is the ability to tune some parameters (stopwords, stemming, lemmantizing, etc.).\n",
    "\n",
    "Here, we prefer not to do stemming. Indeed, this might change considerably the country analysis as well as the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Finding countries in the whole corpus:\n",
    "\n",
    "__Note:__ We only retrieve countries by their name in the Extracted dataset.  \n",
    "_When trying with abbreviations we get a lot of false results because of short keywords and US appears almost everywhere._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycountry import countries\n",
    "\n",
    "def get_country(str):\n",
    "    c = None\n",
    "    try:\n",
    "        if len(str) > 3: # Country names are >3 letters\n",
    "            c = countries.get(name=str.title()).name\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "def retrieve_countries(emails):\n",
    "    mentioned = {}\n",
    "    \n",
    "    def add(country, i):\n",
    "        if country in mentioned:\n",
    "            mentioned[country].append(i)\n",
    "        else:\n",
    "            mentioned[country] = [i]\n",
    "    \n",
    "    for i, email in emails.iteritems():\n",
    "        tokens = run_pipeline(email)\n",
    "        for token in tokens:\n",
    "            country = get_country(token)\n",
    "            if country:\n",
    "                add(country, i)\n",
    "    return mentioned\n",
    "\n",
    "try:\n",
    "    mentioned2 = np.load('mentioned2.npy').item()\n",
    "except:\n",
    "    mentioned2 = retrieve_countries(extracted_emails.Text)\n",
    "    np.save('mentioned2.npy', mentioned2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the polarities using NLTK vader package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    polarities2 = np.load('polarities2.npy')\n",
    "except:\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarities2 = [sid.polarity_scores(email) for email in extracted_emails.Text]\n",
    "    np.save('polarities2.npy', polarities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for country, emails in mentioned2.items():\n",
    "    for email in set(emails):\n",
    "        polarity = polarities2[email]['compound']\n",
    "        result.append([country, polarity])\n",
    "result = pd.DataFrame(result, columns=['Country','Polarity'])\n",
    "sorted = result.groupby('Country').mean().sort_values(by='Polarity')\n",
    "colors = ['b' if pol > 0 else 'r' for pol in sorted.Polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,30))\n",
    "sns.barplot(x='Polarity', \n",
    "            y='Country', \n",
    "            data=result, \n",
    "            order=sorted.index, \n",
    "            palette=colors, \n",
    "            saturation=.4, \n",
    "            errcolor='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the different average of sentiments per country. The gray part represents the standard deviation between each emails. A country with a big country would have a tendency to be _good_ or _bad_, other will tend to be only _good_ or _bad_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Exploring the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let first construct a corpus. A corpus, in this case, is a ensemble of tuple $(i,n_i)$ where $i$ is the $i$th word and $n_i$ is the number of occurence of this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [run_pipeline(text) for text in extracted_emails.Text]\n",
    "dictionary = Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOPICS = 15\n",
    "lda = LdaModel(corpus, num_topics=N_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for no in range(lda.num_topics):\n",
    "    words = ' '.join([w for w, _ in lda.show_topic(no, topn=20)])\n",
    "    print(no+1,':',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over different `N_TOPICS` values, it appears that 15 seems to be a good value for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 BONUS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', header=0, index_col=0)\n",
    "senders = all_emails[['Id','SenderPersonId']]\n",
    "senders = senders[pd.notnull(senders.SenderPersonId)]\n",
    "percent = 100*len(senders)/len(all_emails)\n",
    "print('%.2f' % percent, '% of emails have a valid sender.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for _, emailId, i in senders.itertuples():\n",
    "    for _, emailId_2, j in receivers[receivers.EmailId == emailId].itertuples():\n",
    "        assert emailId == emailId_2 # Check for coherence\n",
    "        G.add_edge(int(i), int(j))\n",
    "partitions = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "communities = {}\n",
    "for k,v in partitions.items():\n",
    "    emails = ' '.join(extracted_emails[all_emails.SenderPersonId == k].Text)\n",
    "    if not v in communities:\n",
    "        communities[v] = [emails]\n",
    "    else:\n",
    "        communities[v].append(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for k,v in communities.items():\n",
    "    text = ' '.join(v)\n",
    "    tokens = run_pipeline(text)\n",
    "    count = Counter(tokens)\n",
    "    most = ' '.join(pd.DataFrame(count.most_common(20))[0]) if count else ''\n",
    "    print(k+1,':',most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 18 communities here and the subjects are not the same as the topic analysis.  \n",
    "This was expected as the LDA model tries to figure out topics but here we only count word occurencies.\n",
    "The most interesting fact is that we can clearly identify discussion topics here but it looks more unclear in the LDA model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [adaenv]",
   "language": "python",
   "name": "Python [adaenv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
